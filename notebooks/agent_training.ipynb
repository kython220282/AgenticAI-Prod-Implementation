{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e09ba66",
   "metadata": {},
   "source": [
    "# Agent Training Notebook\n",
    "\n",
    "This notebook demonstrates how to train and evaluate agents in the Agentic AI framework.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Agent initialization\n",
    "- Environment setup\n",
    "- Training loops\n",
    "- Performance visualization\n",
    "- Model checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b813dc",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d956f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agents import LearningAgent, AutonomousAgent\n",
    "from environment import Simulator\n",
    "from utils import MetricsTracker, Visualizer, setup_logger\n",
    "from config import load_config\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logger('agent_training', level='INFO')\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42f297",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'num_episodes': 100,\n",
    "    'max_steps': 200,\n",
    "    'eval_frequency': 10,\n",
    "    'save_frequency': 50\n",
    "}\n",
    "\n",
    "# Agent configuration\n",
    "agent_config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'discount_factor': 0.95,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 10000\n",
    "}\n",
    "\n",
    "# Environment configuration\n",
    "env_config = {\n",
    "    'num_agents': 1,\n",
    "    'state_dim': 8,\n",
    "    'action_dim': 4,\n",
    "    'max_steps': CONFIG['max_steps'],\n",
    "    'reward_type': 'dense'\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Episodes: {CONFIG['num_episodes']}\")\n",
    "print(f\"  Max steps per episode: {CONFIG['max_steps']}\")\n",
    "print(f\"  Learning rate: {agent_config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9816bc0",
   "metadata": {},
   "source": [
    "## 3. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c188470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = Simulator(env_config)\n",
    "print(f\"✓ Environment created: {env}\")\n",
    "\n",
    "# Create agent\n",
    "agent = LearningAgent(agent_config, name=\"TrainingAgent\")\n",
    "agent.initialize()\n",
    "print(f\"✓ Agent initialized: {agent}\")\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = MetricsTracker(window_size=100)\n",
    "print(\"✓ Metrics tracker ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c7cac",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training for {CONFIG['num_episodes']} episodes...\\n\")\n",
    "\n",
    "for episode in range(CONFIG['num_episodes']):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Agent selects action\n",
    "        action = agent.act(observation)\n",
    "        \n",
    "        # Execute action in environment\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Agent learns\n",
    "        experience = {\n",
    "            'state': observation,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_observation,\n",
    "            'done': done\n",
    "        }\n",
    "        agent.learn(experience)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        observation = next_observation\n",
    "    \n",
    "    # Record metrics\n",
    "    metrics.record('episode_reward', episode_reward, episode=episode)\n",
    "    metrics.record('episode_length', steps, episode=episode)\n",
    "    metrics.record('epsilon', agent.epsilon, episode=episode)\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % CONFIG['eval_frequency'] == 0:\n",
    "        avg_reward = metrics.get_moving_average('episode_reward', 10)\n",
    "        print(f\"Episode {episode + 1}/{CONFIG['num_episodes']} | \"\n",
    "              f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "              f\"ε: {agent.epsilon:.3f} | \"\n",
    "              f\"Memory: {len(agent.memory)}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20086d",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "stats = agent.get_stats()\n",
    "print(\"Agent Statistics:\")\n",
    "print(f\"  Episodes completed: {stats['episodes_completed']}\")\n",
    "print(f\"  Final epsilon: {stats['epsilon']:.3f}\")\n",
    "print(f\"  Memory size: {stats['memory_size']}\")\n",
    "print(f\"  Average reward: {metrics.get_mean('episode_reward'):.2f}\")\n",
    "print(f\"  Best reward: {metrics.get_max('episode_reward'):.2f}\")\n",
    "print(f\"  Average episode length: {metrics.get_mean('episode_length'):.2f}\")\n",
    "\n",
    "# Generate detailed report\n",
    "print(\"\\n\" + metrics.generate_report(['episode_reward', 'episode_length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877aaf5",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47675b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizer\n",
    "viz = Visualizer()\n",
    "\n",
    "# Get data\n",
    "rewards = metrics.get_all('episode_reward')\n",
    "episode_lengths = metrics.get_all('episode_length')\n",
    "epsilons = metrics.get_all('epsilon')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "if len(rewards) >= 20:\n",
    "    moving_avg = np.convolve(rewards, np.ones(20)/20, mode='valid')\n",
    "    plt.plot(range(19, len(rewards)), moving_avg, 'r-', linewidth=2, label='Moving Avg (20)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(episode_lengths)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Episode Length')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epsilons)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Exploration Rate Decay')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/training/training_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualizations saved to ../data/training/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bc013",
   "metadata": {},
   "source": [
    "## 7. Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3697eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save agent checkpoint\n",
    "checkpoint_path = '../data/checkpoints/agent_checkpoint.pkl'\n",
    "agent.save(checkpoint_path)\n",
    "print(f\"✓ Model checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd5d0c",
   "metadata": {},
   "source": [
    "## 8. Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "print(\"Testing trained agent...\")\n",
    "\n",
    "test_rewards = []\n",
    "num_test_episodes = 10\n",
    "\n",
    "# Disable exploration for testing\n",
    "original_epsilon = agent.epsilon\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "for test_ep in range(num_test_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    test_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        test_reward += reward\n",
    "    \n",
    "    test_rewards.append(test_reward)\n",
    "\n",
    "# Restore epsilon\n",
    "agent.epsilon = original_epsilon\n",
    "\n",
    "print(f\"\\nTest Results ({num_test_episodes} episodes):\")\n",
    "print(f\"  Average reward: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"  Std deviation: {np.std(test_rewards):.2f}\")\n",
    "print(f\"  Min reward: {np.min(test_rewards):.2f}\")\n",
    "print(f\"  Max reward: {np.max(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2249361",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✓ Agent initialization and configuration\n",
    "- ✓ Environment setup\n",
    "- ✓ Training loop implementation\n",
    "- ✓ Performance metrics tracking\n",
    "- ✓ Visualization of training progress\n",
    "- ✓ Model checkpointing\n",
    "- ✓ Testing trained agent\n",
    "\n",
    "Next steps:\n",
    "- Experiment with different hyperparameters\n",
    "- Try different agent types\n",
    "- Implement custom reward functions\n",
    "- Add more complex environments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
